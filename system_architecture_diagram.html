<!DOCTYPE html>
<html>
<head>
    <title>Bloody Roar 2 RL System Architecture</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { text-align: center; color: #333; }
        #diagram { text-align: center; margin: 20px 0; }
    </style>
</head>
<body>
    <h1>ğŸ® Bloody Roar 2 RL Training System Architecture</h1>
    
    <div id="diagram">
        <div class="mermaid">
graph TB
    %% Game Layer
    subgraph "ğŸ® Game Layer"
        BH[BizHawk Emulator]
        BR2[Bloody Roar 2 ROM]
        LUA[Lua Input Script]
        AF[actions.txt File]
        
        BH --> BR2
        AF --> LUA
        LUA --> BH
    end
    
    %% Detection Layer
    subgraph "ğŸ‘ï¸ Detection Layer"
        WC[WindowCapture<br/>ğŸ“º Screen Capture]
        HD[HealthDetector<br/>ğŸ’š Yellow Pixel Detection]
        FD[FighterDetector<br/>ğŸ¥Š YOLOv8n Object Detection]
        
        WC --> HD
        WC --> FD
    end
    
    %% State Management Layer
    subgraph "ğŸ“Š State Management"
        HSM[HybridStateManager<br/>ğŸ–¼ï¸ Screenshot + Health History]
        RSM[RoundStateMonitor<br/>âš”ï¸ Round Win/Loss Detection]
        SH[StateHistory<br/>ğŸ“ˆ Temporal Features]
        SN[StateNormalizer<br/>ğŸ¯ Feature Engineering]
        
        HD --> RSM
        HD --> HSM
        WC --> HSM
        HD --> SH
        FD --> SH
        SH --> SN
    end
    
    %% Environment Layer
    subgraph "ğŸŒ RL Environment Hierarchy"
        BASE[BaseSlowRLEnvironment<br/>â±ï¸ 8-second observation windows<br/>ğŸ¯ Health-based rewards]
        
        subgraph "Algorithm-Specific Environments"
            PPO_ENV[PPOSlowRLEnvironment<br/>ğŸ“‹ 11 feature vector<br/>Position + Health + Movement]
            DQN_ENV[DQNSlowRLEnvironment<br/>ğŸ–¼ï¸ Screenshots (8Ã—168Ã—168)<br/>â• Health History (8Ã—4)]
        end
        
        MATCH[MatchRLEnvironment<br/>ğŸ† Best-of-3 Management<br/>ğŸ“Š Match Context Features]
        ARCADE[ArcadeRLEnvironment<br/>ğŸª 8 Opponent Progression<br/>ğŸ”„ Checkpoint System]
        
        BASE --> PPO_ENV
        BASE --> DQN_ENV
        PPO_ENV --> MATCH
        DQN_ENV --> MATCH
        MATCH --> ARCADE
    end
    
    %% Training Layer
    subgraph "ğŸ§  Training Algorithms"
        subgraph "PPO Training"
            PPO_AGENT[PPO Agent<br/>ğŸ­ Actor-Critic<br/>ğŸ”„ Policy Optimization]
            PPO_NET[PPO Networks<br/>ğŸ“Š Feature MLP<br/>ğŸ¯ Action/Value Heads]
            PPO_BUFFER[PPO Buffer<br/>ğŸ’¾ Episode Trajectories<br/>ğŸ“ˆ Advantage Estimation]
        end
        
        subgraph "DQN Training"
            DQN_AGENT[DQN Agent<br/>ğŸ¯ Q-Learning<br/>ğŸ² Îµ-greedy Exploration]
            DQN_NET[DQN Vision Network<br/>ğŸ–¼ï¸ CNN (Screenshots)<br/>ğŸ§  MLP (Health History)<br/>ğŸ”— Feature Fusion]
            DQN_BUFFER[Hybrid Replay Buffer<br/>ğŸ’¾ 50K Transitions<br/>ğŸ–¼ï¸ Screenshot Storage]
            TARGET[Target Network<br/>ğŸ¯ Stable Q-targets<br/>ğŸ”„ Updated every 1000 steps]
        end
    end
    
    %% Action Control
    subgraph "ğŸ® Action Control"
        GC[GameController<br/>âŒ¨ï¸ 10 Discrete Actions<br/>ğŸ“ File-based Communication]
        ACTIONS["ğŸ•¹ï¸ Actions:<br/>left, right, jump, squat<br/>kick, punch, special, throw<br/>block, transform"]
    end
    
    %% Data Flow Connections
    BH -.->|"Screen Pixels"| WC
    
    RSM --> BASE
    SN --> PPO_ENV
    HSM --> DQN_ENV
    
    PPO_ENV --> PPO_AGENT
    DQN_ENV --> DQN_AGENT
    
    PPO_AGENT --> PPO_NET
    PPO_AGENT --> PPO_BUFFER
    
    DQN_AGENT --> DQN_NET
    DQN_AGENT --> DQN_BUFFER
    DQN_NET --> TARGET
    
    PPO_AGENT --> GC
    DQN_AGENT --> GC
    GC --> AF
    GC --> ACTIONS
    
    %% Hierarchical Game Structure
    subgraph "ğŸ—ï¸ Game Hierarchy"
        SUBROUND[Sub-round<br/>âš”ï¸ Single fight until KO<br/>ğŸ“Š 1 RL Episode]
        ROUND[Round<br/>ğŸ”„ Collection of sub-rounds<br/>ğŸ“ˆ Training episodes]
        GMATCH[Match<br/>ğŸ† Best-of-3 rounds<br/>ğŸ¯ Win 2 to advance]
        ARCPROG[Arcade Mode<br/>ğŸª 8 consecutive matches<br/>ğŸ”„ Checkpoint on loss]
        
        SUBROUND --> ROUND
        ROUND --> GMATCH
        GMATCH --> ARCPROG
    end
    
    %% Reward System
    subgraph "ğŸ Reward System"
        HEALTH_REWARD["ğŸ’š Health Rewards<br/>+0.1 Ã— damage_dealt<br/>-0.1 Ã— damage_taken"]
        OUTCOME_BONUS["ğŸ† Outcome Bonuses<br/>+10.0 for wins<br/>-10.0 for losses"]
        
        HD --> HEALTH_REWARD
        RSM --> OUTCOME_BONUS
        HEALTH_REWARD --> BASE
        OUTCOME_BONUS --> BASE
    end
    
    %% Training Loop
    subgraph "ğŸ”„ Training Loop"
        COLLECT[Data Collection<br/>ğŸ® Environment Interaction<br/>ğŸ’¾ Experience Storage]
        UPDATE[Network Updates<br/>ğŸ§  Gradient Descent<br/>ğŸ“Š Loss Optimization]
        EVAL[Evaluation<br/>ğŸ“ˆ Performance Tracking<br/>ğŸ’¾ Checkpoint Saving]
        
        COLLECT --> UPDATE
        UPDATE --> EVAL
        EVAL --> COLLECT
    end
    
    PPO_BUFFER --> UPDATE
    DQN_BUFFER --> UPDATE
    ARCADE --> COLLECT
    
    %% Styling
    classDef gameLayer fill:#e1f5fe
    classDef detectionLayer fill:#f3e5f5
    classDef envLayer fill:#e8f5e8
    classDef trainingLayer fill:#fff3e0
    classDef actionLayer fill:#fce4ec
    
    class BH,BR2,LUA,AF gameLayer
    class WC,HD,FD detectionLayer
    class BASE,PPO_ENV,DQN_ENV,MATCH,ARCADE,HSM,RSM,SH,SN envLayer
    class PPO_AGENT,PPO_NET,PPO_BUFFER,DQN_AGENT,DQN_NET,DQN_BUFFER,TARGET trainingLayer
    class GC,ACTIONS actionLayer
        </div>
    </div>

    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>

    <div style="margin-top: 30px; padding: 20px; background-color: #f5f5f5; border-radius: 5px;">
        <h2>ğŸ“‹ System Overview</h2>
        <ul>
            <li><strong>ğŸ® Game Layer:</strong> BizHawk emulator with Lua script communication</li>
            <li><strong>ğŸ‘ï¸ Detection Layer:</strong> Real-time health and fighter position detection</li>
            <li><strong>ğŸ“Š State Management:</strong> Converts raw game data into RL-ready observations</li>
            <li><strong>ğŸŒ Environment Hierarchy:</strong> Sub-rounds â†’ Rounds â†’ Matches â†’ Arcade</li>
            <li><strong>ğŸ§  Training:</strong> PPO (feature-based) and DQN (vision-based) algorithms</li>
            <li><strong>ğŸ® Action Control:</strong> File-based communication back to game</li>
        </ul>
    </div>
</body>
</html>